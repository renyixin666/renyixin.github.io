---
title: "Transformer å¯è§†åŒ–"
date: 2025-07-23
layout: single
collection: notes
---

## Key Concepts

- Self-attention mechanism
- Positional encoding
- Multi-head attention
- Layer normalization

ğŸ“ [Download Poster (PDF)](/files/paper1.pdf)
